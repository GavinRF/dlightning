<small>
...We almost did a really bad thing
</small>
<br><br>
<p class="first-pgh">There were plenty of reasons we nearly followed through with our terrible plan: SEO ranking, product integration, proliferation of content, brand consistency, just to name a few. Any UX designer will be too-familiar with this very abridged, very demanding list of design-adjacent requirements.
</p>
<p>We needed to finish this checklist and we wanted to do it quickly. With a mountain to climb, our brainstorming arrived quickly at implementing today’s next-greatest digital convenience. A story all-too common lately: AI will make everything easy for us by doing our work. Well, not only was it partially true, but it also turned out to be the fucking problem.
</p>

<h2>The Road to Hell</h2>
<p>Our plan seemed straightforward enough that, in the beginning, it didn’t feel like any real departure from our philosophical ethics. We hoped (and still hope) to offer something of value in the Dlightning *sparks* blog, something that respects the time, intelligence, and humanity of our readers.</p>

<p>After all, the core of our brand, the whole reason we care about what we’re doing here, is that we are human. So-called “users” and their experiences are human. The internet of it all can (and should) be human too, especially our corner of it.</p>

<p>But still, bolstered by naivety and trust in this belief, we let the pressures of meeting industry standards and the temptations of promised ease wash over us. In a moment colored by cleverness and desperation, we turned to AI.</p>

<p>My experience working with language-learning models was extremely limited until this attempt, and I assumed that having taught English to humans, an LLM’s non-human limitations would at least be made up for by its technological advantages (e.g., lacking in intuition but precise in memory). I thought I could teach it to learn.</p>

<p>The goal was to produce articles on common UX design topics, but with a zap of the Dlightning juice—to reconnect over-worn UX design basics back to the basics of human experiencing. There were a lot of topics to cover, so I needed a fast writer whose articles I could quickly edit into finished pieces.</p>

<p>It started simply enough. I asked the chat to hit me with questions, a technique I’d seen somewhere on social media, and pulled a few other tricks to nudge it towards producing a consistent, intelligible, and (the hardest of all) relatable voice. Eventually, I had a so-so brand voice cheat sheet and an article draft on A/B testing a la Cup of Tea.</p>

<p>After hours turning into days of gently molding (and heavily editing the work of) what seemed like a very fragile intelligence, the article was ready to post and my LLM chat was ready to fire off as many more as we needed on other UX topics.</p> 

<p>I should have felt free and clear. I’d done the best I could with the article and we were ready to publish, but we hesitated. There was something wrong.</p>

<p>The A/B testing article itself was a fair piece of writing. It sounded mostly human, especially after extremely thorough edits and rewrites. The tone was exactly where the brand voice guidelines asked it to be, the pacing was good, and it followed a thesis from beginning to end. On paper, nothing should have stopped us from just posting it. After all, it wasn’t some world-shifting think-piece—it was a blog article.</p>

<p>Still, we had dumped so much of our own human experiences into every stage of nudging the LLM and into each generative iteration of the article, but not even word-by-word amendments could erase the slow realization of apparent soullessness.
</p>

<h2>Sans Soul</h2>
<p>“Soullessness” has some pretty dramatic connotations, all of which I hope you can leave aside here. What I mean is that, even when continuously injected with humanity, the LLM “author” had a complete lack of having an experience. It was completely without experiencing of any kind.</p>

<p>It didn’t experience learning to shape a voice or generate an article, and it certainly didn’t experience learning from mistakes and making new connections. This seemingly-capable “author” had no idea what it was doing, and to boot, it had no idea that it had no idea. Nothing unfolded, and with no unfolding comes no discovery. Just endlessly recursive ignorance.</p>

<p>The problem wasn’t that the AI model was a kind of ignorance ouroboros (which it most certainly is). The problem was how close it came to seeming real while masking its eerie soullessness, like a mime without makeup whose act never ends.</p>

<p>The point of our brand’s existence and this blog’s whole deal is about bringing more soul into the design world, and we almost blew it all up over meaningless content. As far as writing and UX design go, it’s safe to say we’re fairly perceptive and competent people, but still, all that stopped us from undermining the ethics of the whole Dlightning project was a gut feeling.</p>

<p>There’s no ultimate moral I’m trying to press here, but you don’t need me to feed you one for the humanness of this article to be fully overt. If nothing else, while I write it, I am experiencing the passage of time, witnessing high and low moments layer over each other as they become the past. I am building an understanding of this past through the lens of my present moment, but my ultimately inept LLM writer can only “see” everything at once.</p>

<p>At first, as I mentioned before, I’d assumed that the AI’s lack of intuition might be offset by its ability to recall the past with pinpoint accuracy. The issue with this assumption was that the AI never could differentiate the past from the present to begin with. And frustratingly (or thankfully), this meant that, ignorant of the passage of time, it couldn’t tell a story.</p>

<p>Everything the LLM produced was a mere shadow of a story, a scattered sequence of plot points tied together by nothing but superficial connections. If a good story can be described as alive, everything the AI “wrote” was dead on arrival.
</p>

<h2>The Real Monster</h2>
<p>There’s really only one moment in Mary Shelley’s Frankenstein that makes it science fiction: the singular instant where the corpse-monster was imbued with life. Up to that point, it’s just a story of a questionable doctor doing gross things. But as to tinkering with the monster that was my blog post on A/B testing, I had no hope of reanimating these sewn-together chunks of dead story-flesh (undoubtedly and literally stolen chunks of articles on A/B testing already published online by other, human, writers).</p>

<p>My apologies for the gruesome imagery, but at the end of the day, I did feel a sense of disgust. That’s not to say I find LLMs at all disgusting for what they are. That would be like Dr. Frankenstein getting upset by his scalpel and stitch-sewing tools. The accurate parallel would be Dr. Frankenstein, instead, realizing the vileness of his actions as he built the monster–how he was using his scalpel before flipping the switch.</p>

<p>It could be argued for many literary reasons that the monster in the book was never alive, even after its reanimation. Using an LLM to hack together the necessary elements of this article (optimal SEO, keyword-searchable, voice-consistent, etc.) would never result in a living story worth publishing, even with worthiness-standards calibrated to “blog”.</p>

<p>As a (fictional) human being capable of perceiving the passage of time, Dr. Frankenstein should have tapped into his foresight. And so could we have done before embarking on our LLM experiment. Unlike the doctor, we, thankfully, paused before endowing our monster with the ability to go out into the world, alive or not.
</p>

<h2>Sensing Aliveness</h2>
<p>All in all, this experiment using AI as an author has been an excellent window into precisely why our core mission is humanity in UX design. It might be difficult to describe “life”, but we know exactly what it is when we feel its absence.</p> 

<p>Similarly, it’s been difficult for people to describe exactly how the internet of things comes into our lives as a draining force, but it’s unbearably present when it’s over-quantified, hyper-standardized, and coming from non-human sources. In an ideal world, the UX design field has always been meant to seek solutions to these fuzzy yet overwhelming problems.</p>

<p>Hopefully this article, this unfolding story, demonstrates the “humanness” that’s so critical in everything we do as interface designers, as app builders, web designers, researchers, testing pros, et al. Every single earnest attempt to embody, in our work, that which is ultimately “unqualifiable” (i.e., the aliveness of a human-written piece that leaves AI-written articles feeling dead) is a huge win.</p>

<p>We can all empathize with Mary Shelley’s mad scientist. We can even feel for his monster. We’re all messing around with LLMs these days, doing wacky experiments and testing ideas, none of which is the “bad thing” we almost did. Sewing pieces of different corpses together, albeit horrific and disgusting, isn’t explicitly “bad”.</p>

<p>Victor Frankenstein arguably only did one “bad” thing in the story, the very same thing we almost did by posting our AI-generated article. This small but dangerous ambiguity he committed, while present in Shelley’s novel, is more succinctly and memorably punctuated in the 1931 Boris Karloff movie-portrayal. As the monster first opened his eyes and began to move, the doctor exclaimed–as truth–what we discover is actually far from it:</p>

<blockquote><i>“It’s alive! It’s alive!”</i></blockquote>

<p>Dr. Frankenstein’s downfall here wasn't that he was wrong or that he lied, it was that he believed himself. And after a novel’s-worth of the monster’s many attempts to humanize himself (the film kinda just glosses over the last 200 pages of the book here), no amount of imitation could bridge the impossible gap from monstrosity to humanity.</p>

<p>For us to believe that AI can do what we do is believing that we can truly bring life to our monster. Whatever we discover these LLMs to be over time, the work of anyone in the field of user experience relies on people. Period.
</p>

<h2>Reason Enough</h2>
<p>Let me state, bluntly, the “bad thing” we almost did: we almost replaced ourselves.</p>

<p>We very nearly believed that our monster was alive, that something of value in the business of human-to-human interfacing through technology could be machine-generated.</p>

<p>To be clear, AI could never “be bad”. And using LLMs, not a crime. It's really not about moralizing these new machine entities or their use, but rather about valuing our own humanity enough to recognize that AIs are simply not human. That’s enough cause to think twice in a field that’s all about interfacing with actual human beings. Using an LLM to create UX content was, in the plainest sense, antithetical to UX design itself.</p>

<p>It's impossible to ask a real-world designer working on real-world projects to romanticize the field by abandoning real-world demands. SEO ranking is important; content is necessary; products need advertising. The items on the to-do list that led us down the AI path are unavoidable, but it’s crucial we avoid replacing ourselves in order to meet these demands.</p>

<p>Reinvesting trust into the human element will always pay off in the end anyway, because it will always keep the story going. The monster eventually kills his creator before condemning himself to freeze to death. The story ends. Moreover, the monster is the one that ends his story. It could never have endured the way the human spirit has been known to do, and there is really only one requirement for having human spirit.</p>

<p>I'm sure that some readers of this article will be AIs (spellcheck, anyone?), but to my fellow homo sapiens–let's do our best to remember that this work doesn't exist without us, without each other. Why? It’s not complicated. It’s that we simply are human, and that’s reason enough.
</p>